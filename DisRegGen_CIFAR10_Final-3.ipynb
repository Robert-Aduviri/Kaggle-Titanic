{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DisRegGen-CIFAR10_Final.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "wQvY-qYdTv-x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# <center>Discriminative Regularize Generative Model for CIFAR10 </center>"
      ]
    },
    {
      "metadata": {
        "id": "-Rj9KcbOTv-z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ]
    },
    {
      "metadata": {
        "id": "XTv4faCZTv-0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "527885ae-2da5-416c-c097-98ee6f293576",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526728855320,
          "user_tz": -180,
          "elapsed": 52422,
          "user": {
            "displayName": "Alfredo Alejandro De la Fuente Briceño",
            "photoUrl": "//lh3.googleusercontent.com/-KuSUlXYxUa0/AAAAAAAAAAI/AAAAAAAAACI/FWR29NQO7b8/s50-c-k-no/photo.jpg",
            "userId": "115022533346902252990"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade torch torchvision\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "#from google.colab import files\n",
        "\n",
        "#Set random seed \n",
        "torch.manual_seed(512)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/43/380514bd9663f1bf708abeb359b8b48d3fabb1c8e95bb3427a980a064c57/torch-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (484.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 484.0MB 27kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x5cd40000 @  0x7f0e456901c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 15.7MB/s \n",
            "\u001b[?25hRequirement not upgraded as not directly required: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/4b/8b54ab9d37b93998c81b364557dff9f61972c0f650efa0ceaf470b392740/Pillow-5.1.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 16.4MB/s \n",
            "\u001b[?25hRequirement not upgraded as not directly required: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.3)\n",
            "Installing collected packages: torch, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.1.0 torch-0.4.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f190e6f1e90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "upjqkt52z27T",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class preTrainedModel(nn.Module):\n",
        "  \n",
        "    def __init__(self):\n",
        "      \n",
        "      super(preTrainedModel,self).__init__()\n",
        "      \n",
        "      vgg_model = torchvision.models.vgg16(pretrained=True)\t\t\n",
        "      self.Conv1 = nn.Sequential(*list(vgg_model.features.children())[0:4])\n",
        "      #self.Conv2 = nn.Sequential(*list(vgg_model.features.children())[4:9]) \n",
        "      #self.Conv3 = nn.Sequential(*list(vgg_model.features.children())[9:16])\n",
        "      #self.upSample1 = nn.Upsample(scale_factor=2)\n",
        "      #self.upSample2 = nn.Upsample(scale_factor=4)\n",
        "\n",
        "    def forward(self,x):\n",
        "      out1 = self.Conv1(x)\n",
        "      #out2 = self.Conv2(out1)\n",
        "      #out3 = self.Conv3(out2)\n",
        "      ###### up sampling to create output with the same size\n",
        "      #out2 = self.upSample1(out2)\n",
        "      #out3 = self.upSample2(out3)\n",
        "      #concat_features = torch.cat([out1, out2, out3], 1)\n",
        "      return out1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SPpn7-HtTv_D",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "df1eb957-b421-45c9-d973-d4315385ae53",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526729009116,
          "user_tz": -180,
          "elapsed": 150022,
          "user": {
            "displayName": "Alfredo Alejandro De la Fuente Briceño",
            "photoUrl": "//lh3.googleusercontent.com/-KuSUlXYxUa0/AAAAAAAAAAI/AAAAAAAAACI/FWR29NQO7b8/s50-c-k-no/photo.jpg",
            "userId": "115022533346902252990"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Load model \n",
        "vgg19 = preTrainedModel().eval().cuda()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /content/.torch/models/vgg16-397923af.pth\n",
            "1.2%"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "DsoOKn4BTv_F",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9193b56f-8f76-4d96-cfeb-3a45d6d9174f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526732111041,
          "user_tz": -180,
          "elapsed": 22557,
          "user": {
            "displayName": "Alfredo Alejandro De la Fuente Briceño",
            "photoUrl": "//lh3.googleusercontent.com/-KuSUlXYxUa0/AAAAAAAAAAI/AAAAAAAAACI/FWR29NQO7b8/s50-c-k-no/photo.jpg",
            "userId": "115022533346902252990"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#Get the CIFAR10 train images \n",
        "cifar = datasets.CIFAR10('./data/cifar/', train = True, download = True)\n",
        "\n",
        "# Organize training data in batches, \n",
        "# normalize them to have values between [-1, 1] (?)\n",
        "\n",
        "train_images = torch.utils.data.DataLoader ( datasets.CIFAR10('./data/cifar/', train = True, download=False,\n",
        "                               transform=transforms.Compose([\n",
        "                               #transforms.Resize(64), \n",
        "                               #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                               transforms.ToTensor(),])) , \n",
        "                               batch_size = 8, shuffle = True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0N6c7NtWTv_Z",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7ae6d969-16df-4877-93ef-9a58cce95e8d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526732111666,
          "user_tz": -180,
          "elapsed": 554,
          "user": {
            "displayName": "Alfredo Alejandro De la Fuente Briceño",
            "photoUrl": "//lh3.googleusercontent.com/-KuSUlXYxUa0/AAAAAAAAAAI/AAAAAAAAACI/FWR29NQO7b8/s50-c-k-no/photo.jpg",
            "userId": "115022533346902252990"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "upsampling = nn.Upsample(size=256)\n",
        "for batch_idx, (data,_) in enumerate(train_images):    \n",
        "    out = vgg19(upsampling(data.cuda()))\n",
        "    print(out.size())\n",
        "    print(data.size())\n",
        "    #print(data[0])\n",
        "    break"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8, 64, 256, 256])\n",
            "torch.Size([8, 3, 32, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AsFG7sSkTv_d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "We will use the arquitecture suggested by [Radford et al](https://arxiv.org/abs/1511.06434) for both the encoder and decoder. With convolutional layers in the encoder and fractionally-strided  convolutions  in  the  decoder.   In  each convolutional layer in the encoder we double the number of filters present in the previous layer and use a convolutional stride of 2.  In each convolutional layer in the decoder we use a fractional stride of 2 and halve the number of filters on each layer."
      ]
    },
    {
      "metadata": {
        "id": "BxB4_CTcTv_d",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class VAE( nn.Module ):\n",
        "\n",
        "    def __init__ ( self, image_size ,  hidden_dim , encoding_dim ):\n",
        "        \n",
        "        super( VAE, self ).__init__()\n",
        "        \n",
        "        self.encoding_dim = encoding_dim\n",
        "        self.image_size = image_size\n",
        "        self.hidden_dim = hidden_dim \n",
        "        \n",
        "        # Decoder - Fractional strided convolutional layers\n",
        "        self.decoder  = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 4, 1, 0, bias = False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(32, 3, 4, 2, 1, bias = False),\n",
        "            nn.Sigmoid() # nn.Tanh()  \n",
        "        )\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, 2, 1, bias = False),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(32, 64, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 0, bias = False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "        # Fully-connected layers\n",
        "        self.fc1 = nn.Linear(256, self.hidden_dim)\n",
        "        self.fc21 = nn.Linear(self.hidden_dim, self.encoding_dim)\n",
        "        self.fc22 = nn.Linear(self.hidden_dim, self.encoding_dim)\n",
        "        self.fc3 = nn.Linear(self.encoding_dim, self.hidden_dim)\n",
        "        self.fc4 = nn.Linear(self.hidden_dim, 256)\n",
        "    \n",
        "    def decode (self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        h4 = F.sigmoid(self.fc4(h3))\n",
        "        return self.decoder( h4.view(z.size(0),-1,1,1) ) \n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # Encode \n",
        "        encoded = F.relu(self.fc1( self.encoder(x).view(x.size(0), -1) ) )\n",
        "        \n",
        "        #Obtain mu and logvar\n",
        "        mu = self.fc21( encoded )\n",
        "        logvar = self.fc22 ( encoded )\n",
        "        \n",
        "        #Reparametrization trick\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = eps.mul(std).add_(mu)\n",
        "        \n",
        "        # Decode \n",
        "        decoded = self.decode(z)\n",
        "\n",
        "        # return decoded, mu, logvar\n",
        "        return decoded, mu , logvar\n",
        "\n",
        "\n",
        "upsampling = nn.Upsample(size=256)\n",
        "sigmoid = nn.Sigmoid()\n",
        "\n",
        "# Reconstruction + KL divergence losses summed over all elements and batch\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    \n",
        "    d1_recon_x = sigmoid(vgg19(upsampling( recon_x )))\n",
        "    d1_x = sigmoid(vgg19( upsampling( x )))\n",
        "\n",
        "    L1 = F.mse_loss(d1_recon_x, d1_x, size_average=False)\n",
        "    \n",
        "    del d1_recon_x \n",
        "    del d1_x\n",
        "    \n",
        "    return BCE + KLD + L1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xa14nLhnTv_f",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "fb6d3fa2-34a5-4f4b-dc74-06b644aa0504",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526733558939,
          "user_tz": -180,
          "elapsed": 72496,
          "user": {
            "displayName": "Alfredo Alejandro De la Fuente Briceño",
            "photoUrl": "//lh3.googleusercontent.com/-KuSUlXYxUa0/AAAAAAAAAAI/AAAAAAAAACI/FWR29NQO7b8/s50-c-k-no/photo.jpg",
            "userId": "115022533346902252990"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "#Define model\n",
        "model = VAE( 32, 100, 20 ).cuda()\n",
        "#model.load_state_dict(torch.load('save_checkpoint_epoch_69.pth'))\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "#Train model\n",
        "def train(epoch):\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_images):\n",
        "        data = Variable(data).cuda()\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_images.dataset),\n",
        "                100. * batch_idx / len(train_images),\n",
        "                loss.item() / len(data)))\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(train_images.dataset)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f4bb7fb1-9f8c-4c05-8ea6-9aa13cba528e\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-f4bb7fb1-9f8c-4c05-8ea6-9aa13cba528e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving save_checkpoint_epoch_69.pth to save_checkpoint_epoch_69.pth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GJFbZKvNTv_h",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 2159
        },
        "outputId": "f3152711-6b27-4c1f-8255-1c89aec2d371",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526735322524,
          "user_tz": -180,
          "elapsed": 1706493,
          "user": {
            "displayName": "Alfredo Alejandro De la Fuente Briceño",
            "photoUrl": "//lh3.googleusercontent.com/-KuSUlXYxUa0/AAAAAAAAAAI/AAAAAAAAACI/FWR29NQO7b8/s50-c-k-no/photo.jpg",
            "userId": "115022533346902252990"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "num_epochs = 2\n",
        "for epoch in range(1,num_epochs):\n",
        "    train(epoch)    "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 7168.814453\n",
            "Train Epoch: 1 [400/50000 (1%)]\tLoss: 8286.398438\n",
            "Train Epoch: 1 [800/50000 (2%)]\tLoss: 8785.705078\n",
            "Train Epoch: 1 [1200/50000 (2%)]\tLoss: 8239.338867\n",
            "Train Epoch: 1 [1600/50000 (3%)]\tLoss: 8859.353516\n",
            "Train Epoch: 1 [2000/50000 (4%)]\tLoss: 7408.976562\n",
            "Train Epoch: 1 [2400/50000 (5%)]\tLoss: 6711.461914\n",
            "Train Epoch: 1 [2800/50000 (6%)]\tLoss: 6423.336426\n",
            "Train Epoch: 1 [3200/50000 (6%)]\tLoss: 8909.876953\n",
            "Train Epoch: 1 [3600/50000 (7%)]\tLoss: 6957.336914\n",
            "Train Epoch: 1 [4000/50000 (8%)]\tLoss: 8573.662109\n",
            "Train Epoch: 1 [4400/50000 (9%)]\tLoss: 6625.413086\n",
            "Train Epoch: 1 [4800/50000 (10%)]\tLoss: 8149.950195\n",
            "Train Epoch: 1 [5200/50000 (10%)]\tLoss: 8172.897461\n",
            "Train Epoch: 1 [5600/50000 (11%)]\tLoss: 7469.384766\n",
            "Train Epoch: 1 [6000/50000 (12%)]\tLoss: 7907.474121\n",
            "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 7876.250977\n",
            "Train Epoch: 1 [6800/50000 (14%)]\tLoss: 6129.447266\n",
            "Train Epoch: 1 [7200/50000 (14%)]\tLoss: 8253.538086\n",
            "Train Epoch: 1 [7600/50000 (15%)]\tLoss: 6731.223145\n",
            "Train Epoch: 1 [8000/50000 (16%)]\tLoss: 8806.402344\n",
            "Train Epoch: 1 [8400/50000 (17%)]\tLoss: 7180.415527\n",
            "Train Epoch: 1 [8800/50000 (18%)]\tLoss: 7425.602539\n",
            "Train Epoch: 1 [9200/50000 (18%)]\tLoss: 6567.501953\n",
            "Train Epoch: 1 [9600/50000 (19%)]\tLoss: 7207.916016\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tLoss: 8506.325195\n",
            "Train Epoch: 1 [10400/50000 (21%)]\tLoss: 6993.658691\n",
            "Train Epoch: 1 [10800/50000 (22%)]\tLoss: 8062.822266\n",
            "Train Epoch: 1 [11200/50000 (22%)]\tLoss: 8968.122070\n",
            "Train Epoch: 1 [11600/50000 (23%)]\tLoss: 6767.536133\n",
            "Train Epoch: 1 [12000/50000 (24%)]\tLoss: 6801.605469\n",
            "Train Epoch: 1 [12400/50000 (25%)]\tLoss: 8293.904297\n",
            "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 7947.135742\n",
            "Train Epoch: 1 [13200/50000 (26%)]\tLoss: 7503.587891\n",
            "Train Epoch: 1 [13600/50000 (27%)]\tLoss: 7215.844238\n",
            "Train Epoch: 1 [14000/50000 (28%)]\tLoss: 6838.505859\n",
            "Train Epoch: 1 [14400/50000 (29%)]\tLoss: 8546.557617\n",
            "Train Epoch: 1 [14800/50000 (30%)]\tLoss: 8747.061523\n",
            "Train Epoch: 1 [15200/50000 (30%)]\tLoss: 7985.065918\n",
            "Train Epoch: 1 [15600/50000 (31%)]\tLoss: 6886.908691\n",
            "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 7183.043457\n",
            "Train Epoch: 1 [16400/50000 (33%)]\tLoss: 9121.001953\n",
            "Train Epoch: 1 [16800/50000 (34%)]\tLoss: 7082.146484\n",
            "Train Epoch: 1 [17200/50000 (34%)]\tLoss: 8672.728516\n",
            "Train Epoch: 1 [17600/50000 (35%)]\tLoss: 8872.246094\n",
            "Train Epoch: 1 [18000/50000 (36%)]\tLoss: 8165.984375\n",
            "Train Epoch: 1 [18400/50000 (37%)]\tLoss: 7654.452148\n",
            "Train Epoch: 1 [18800/50000 (38%)]\tLoss: 7865.013184\n",
            "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 5984.514160\n",
            "Train Epoch: 1 [19600/50000 (39%)]\tLoss: 7576.007324\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tLoss: 7925.769043\n",
            "Train Epoch: 1 [20400/50000 (41%)]\tLoss: 8820.723633\n",
            "Train Epoch: 1 [20800/50000 (42%)]\tLoss: 7091.052734\n",
            "Train Epoch: 1 [21200/50000 (42%)]\tLoss: 6961.408691\n",
            "Train Epoch: 1 [21600/50000 (43%)]\tLoss: 7479.711914\n",
            "Train Epoch: 1 [22000/50000 (44%)]\tLoss: 7406.149902\n",
            "Train Epoch: 1 [22400/50000 (45%)]\tLoss: 8694.740234\n",
            "Train Epoch: 1 [22800/50000 (46%)]\tLoss: 8256.296875\n",
            "Train Epoch: 1 [23200/50000 (46%)]\tLoss: 7446.453613\n",
            "Train Epoch: 1 [23600/50000 (47%)]\tLoss: 6881.537598\n",
            "Train Epoch: 1 [24000/50000 (48%)]\tLoss: 7125.672852\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [24400/50000 (49%)]\tLoss: 7598.856445\n",
            "Train Epoch: 1 [24800/50000 (50%)]\tLoss: 7275.869141\n",
            "Train Epoch: 1 [25200/50000 (50%)]\tLoss: 6732.260254\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 8605.176758\n",
            "Train Epoch: 1 [26000/50000 (52%)]\tLoss: 7506.098633\n",
            "Train Epoch: 1 [26400/50000 (53%)]\tLoss: 6038.569824\n",
            "Train Epoch: 1 [26800/50000 (54%)]\tLoss: 6971.148438\n",
            "Train Epoch: 1 [27200/50000 (54%)]\tLoss: 7233.091797\n",
            "Train Epoch: 1 [27600/50000 (55%)]\tLoss: 6783.173340\n",
            "Train Epoch: 1 [28000/50000 (56%)]\tLoss: 7911.638672\n",
            "Train Epoch: 1 [28400/50000 (57%)]\tLoss: 8042.156250\n",
            "Train Epoch: 1 [28800/50000 (58%)]\tLoss: 7496.097656\n",
            "Train Epoch: 1 [29200/50000 (58%)]\tLoss: 7708.732910\n",
            "Train Epoch: 1 [29600/50000 (59%)]\tLoss: 7672.047363\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tLoss: 7319.247070\n",
            "Train Epoch: 1 [30400/50000 (61%)]\tLoss: 6921.179199\n",
            "Train Epoch: 1 [30800/50000 (62%)]\tLoss: 8524.701172\n",
            "Train Epoch: 1 [31200/50000 (62%)]\tLoss: 7772.747559\n",
            "Train Epoch: 1 [31600/50000 (63%)]\tLoss: 6067.373047\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 7205.217773\n",
            "Train Epoch: 1 [32400/50000 (65%)]\tLoss: 6731.922363\n",
            "Train Epoch: 1 [32800/50000 (66%)]\tLoss: 8629.551758\n",
            "Train Epoch: 1 [33200/50000 (66%)]\tLoss: 7544.478027\n",
            "Train Epoch: 1 [33600/50000 (67%)]\tLoss: 7272.670410\n",
            "Train Epoch: 1 [34000/50000 (68%)]\tLoss: 6702.479980\n",
            "Train Epoch: 1 [34400/50000 (69%)]\tLoss: 6713.267578\n",
            "Train Epoch: 1 [34800/50000 (70%)]\tLoss: 7247.295898\n",
            "Train Epoch: 1 [35200/50000 (70%)]\tLoss: 7469.322754\n",
            "Train Epoch: 1 [35600/50000 (71%)]\tLoss: 6183.803711\n",
            "Train Epoch: 1 [36000/50000 (72%)]\tLoss: 7155.742188\n",
            "Train Epoch: 1 [36400/50000 (73%)]\tLoss: 7554.377930\n",
            "Train Epoch: 1 [36800/50000 (74%)]\tLoss: 7759.929688\n",
            "Train Epoch: 1 [37200/50000 (74%)]\tLoss: 6030.073242\n",
            "Train Epoch: 1 [37600/50000 (75%)]\tLoss: 6813.784180\n",
            "Train Epoch: 1 [38000/50000 (76%)]\tLoss: 7491.213867\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 7966.554199\n",
            "Train Epoch: 1 [38800/50000 (78%)]\tLoss: 8091.885254\n",
            "Train Epoch: 1 [39200/50000 (78%)]\tLoss: 7324.189453\n",
            "Train Epoch: 1 [39600/50000 (79%)]\tLoss: 6935.903809\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tLoss: 8962.474609\n",
            "Train Epoch: 1 [40400/50000 (81%)]\tLoss: 7523.609863\n",
            "Train Epoch: 1 [40800/50000 (82%)]\tLoss: 7438.920410\n",
            "Train Epoch: 1 [41200/50000 (82%)]\tLoss: 7627.952148\n",
            "Train Epoch: 1 [41600/50000 (83%)]\tLoss: 8627.314453\n",
            "Train Epoch: 1 [42000/50000 (84%)]\tLoss: 8157.561035\n",
            "Train Epoch: 1 [42400/50000 (85%)]\tLoss: 6776.231934\n",
            "Train Epoch: 1 [42800/50000 (86%)]\tLoss: 8545.683594\n",
            "Train Epoch: 1 [43200/50000 (86%)]\tLoss: 8715.597656\n",
            "Train Epoch: 1 [43600/50000 (87%)]\tLoss: 8288.505859\n",
            "Train Epoch: 1 [44000/50000 (88%)]\tLoss: 8253.030273\n",
            "Train Epoch: 1 [44400/50000 (89%)]\tLoss: 6469.333496\n",
            "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 7361.999512\n",
            "Train Epoch: 1 [45200/50000 (90%)]\tLoss: 6608.365234\n",
            "Train Epoch: 1 [45600/50000 (91%)]\tLoss: 6865.545898\n",
            "Train Epoch: 1 [46000/50000 (92%)]\tLoss: 8059.375977\n",
            "Train Epoch: 1 [46400/50000 (93%)]\tLoss: 7720.807617\n",
            "Train Epoch: 1 [46800/50000 (94%)]\tLoss: 8467.052734\n",
            "Train Epoch: 1 [47200/50000 (94%)]\tLoss: 6397.742188\n",
            "Train Epoch: 1 [47600/50000 (95%)]\tLoss: 6864.474609\n",
            "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 8094.515625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [48400/50000 (97%)]\tLoss: 7949.773438\n",
            "Train Epoch: 1 [48800/50000 (98%)]\tLoss: 7258.249512\n",
            "Train Epoch: 1 [49200/50000 (98%)]\tLoss: 8273.482422\n",
            "Train Epoch: 1 [49600/50000 (99%)]\tLoss: 8559.158203\n",
            "====> Epoch: 1 Average loss: 7561.2935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WYzZZVPjTv_u",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "torch.save(model.cpu().state_dict(), \"save_checkpoint_epoch_70.pth\")\n",
        "files.download(\"save_checkpoint_epoch_70.pth\")\n",
        "               \n",
        "               "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4lxOTzHYQKGu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "outputId": "e71752c5-4e9b-403f-e72c-0dc58a1c6ca3",
        "executionInfo": {
          "status": "error",
          "timestamp": 1526735730327,
          "user_tz": -180,
          "elapsed": 598,
          "user": {
            "displayName": "Alfredo Alejandro De la Fuente Briceño",
            "photoUrl": "//lh3.googleusercontent.com/-KuSUlXYxUa0/AAAAAAAAAAI/AAAAAAAAACI/FWR29NQO7b8/s50-c-k-no/photo.jpg",
            "userId": "115022533346902252990"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "        sample = torch.randn(64, 20)\n",
        "        sample = model.decode(sample)\n",
        "        #torch.save(model.cpu().state_dict(), \"./save_checkpoint_epoch_\"+str(epoch)+\".pth\")\n",
        "        #files.download(\"./save_checkpoint_epoch_\"+str(epoch)+\".pth\")\n",
        "        torchvision.utils.save_image(sample.view(64, 3, 32, 32),'./sample_' + str(epoch) + '.png')\n",
        "        files.download('./sample_' + str(epoch) + '.png')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-7218433a69ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m#torch.save(model.cpu().state_dict(), \"./save_checkpoint_epoch_\"+str(epoch)+\".pth\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m#files.download(\"./save_checkpoint_epoch_\"+str(epoch)+\".pth\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'./sample_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./sample_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/utils.py\u001b[0m in \u001b[0;36msave_image\u001b[0;34m(tensor, filename, nrow, padding, normalize, range, scale_each, pad_value)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mndarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   1704\u001b[0m         \"\"\"\n\u001b[1;32m   1705\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mresized\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1706\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1707\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mrequested\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpixels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m            \u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mpreinit\u001b[0;34m()\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_initialized\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBmpImagePlugin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/JpegImagePlugin.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTiffImagePlugin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_binary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mi8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi16be\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mi16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mJpegPresets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpresets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1826\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_save_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTiffImageFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_save_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1828\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_extensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTiffImageFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\".tif\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".tiff\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_mime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTiffImageFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"image/tiff\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'PIL.Image' has no attribute 'register_extensions'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "qvQl9CwvYNRb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "torch.save(sample.cpu(), \"sample_70.pth\")\n",
        "files.download(\"sample_70.pth\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nU4DWrDXaMtT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ebde9c9b-54d1-4f19-d3fc-aeba3999c08e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526652682691,
          "user_tz": -180,
          "elapsed": 636,
          "user": {
            "displayName": "Alfredo Alejandro De la Fuente Briceño",
            "photoUrl": "//lh3.googleusercontent.com/-KuSUlXYxUa0/AAAAAAAAAAI/AAAAAAAAACI/FWR29NQO7b8/s50-c-k-no/photo.jpg",
            "userId": "115022533346902252990"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sample.size()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "COzRhVNGYUu6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}