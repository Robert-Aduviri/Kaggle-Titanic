{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DisRegGen_CIFAR10_Final.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Robert-Alonso/Kaggle-Titanic/blob/master/DisRegGen_CIFAR10_Final.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "wQvY-qYdTv-x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# <center>Discriminative Regularize Generative Model for CIFAR10 </center>"
      ]
    },
    {
      "metadata": {
        "id": "-Rj9KcbOTv-z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ]
    },
    {
      "metadata": {
        "id": "XTv4faCZTv-0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "8fd0e08f-677d-4087-d8bf-8b6a5a1f64c1"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade torch torchvision\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "from google.colab import files\n",
        "\n",
        "#Set random seed \n",
        "torch.manual_seed(512)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/43/380514bd9663f1bf708abeb359b8b48d3fabb1c8e95bb3427a980a064c57/torch-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (484.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 484.0MB 24kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x5cedc000 @  0x7fcedd4d51c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 20.5MB/s \n",
            "\u001b[?25hRequirement not upgraded as not directly required: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/4b/8b54ab9d37b93998c81b364557dff9f61972c0f650efa0ceaf470b392740/Pillow-5.1.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 11.6MB/s \n",
            "\u001b[?25hRequirement not upgraded as not directly required: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.3)\n",
            "Installing collected packages: torch, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.1.0 torch-0.4.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fcdc9a1fed0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "upjqkt52z27T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class preTrainedModel(nn.Module):\n",
        "  \n",
        "    def __init__(self):\n",
        "      \n",
        "      super(preTrainedModel,self).__init__()\n",
        "      \n",
        "      vgg_model = torchvision.models.vgg16(pretrained=True)\t\t\n",
        "      self.Conv1 = nn.Sequential(*list(vgg_model.features.children())[0:4])\n",
        "      self.Conv2 = nn.Sequential(*list(vgg_model.features.children())[4:9]) \n",
        "      #self.Conv3 = nn.Sequential(*list(vgg_model.features.children())[9:16])\n",
        "      #self.upSample1 = nn.Upsample(scale_factor=2)\n",
        "      #self.upSample2 = nn.Upsample(scale_factor=4)\n",
        "      \n",
        "      for param in self.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    def forward(self,x):\n",
        "      out1 = self.Conv1(x)\n",
        "      out2 = self.Conv2(out1)\n",
        "      #out3 = self.Conv3(out2)\n",
        "      ###### up sampling to create output with the same size\n",
        "      #out2 = self.upSample1(out2)\n",
        "      #out3 = self.upSample2(out3)\n",
        "      #concat_features = torch.cat([out1, out2, out3], 1)\n",
        "      return (out1,out2)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DEEZv-8R4w-z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Vgg16(nn.Module):\n",
        "  \n",
        "    def __init__(self):\n",
        "        super(Vgg16, self).__init__()\n",
        "        features = torchvision.models.vgg16(pretrained=True).features\n",
        "        self.to_relu_1_2 = nn.Sequential() \n",
        "        self.to_relu_2_2 = nn.Sequential() \n",
        "        self.to_relu_3_3 = nn.Sequential()\n",
        "        self.to_relu_4_3 = nn.Sequential()\n",
        "\n",
        "        for x in range(4):\n",
        "            self.to_relu_1_2.add_module(str(x), features[x])\n",
        "        for x in range(4, 9):\n",
        "            self.to_relu_2_2.add_module(str(x), features[x])\n",
        "        for x in range(9, 16):\n",
        "            self.to_relu_3_3.add_module(str(x), features[x])\n",
        "        for x in range(16, 23):\n",
        "            self.to_relu_4_3.add_module(str(x), features[x])\n",
        "        \n",
        "        # don't need the gradients, just want the features\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.to_relu_1_2(x)\n",
        "        h_relu_1_2 = h\n",
        "        h = self.to_relu_2_2(h)\n",
        "        h_relu_2_2 = h\n",
        "        h = self.to_relu_3_3(h)\n",
        "        h_relu_3_3 = h\n",
        "        h = self.to_relu_4_3(h)\n",
        "        h_relu_4_3 = h\n",
        "        out = (h_relu_1_2, h_relu_2_2, h_relu_3_3, h_relu_4_3)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SPpn7-HtTv_D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c0da45ca-95f0-41c0-9166-22f0ea7f72f5"
      },
      "cell_type": "code",
      "source": [
        "#Load model \n",
        "vgg16 = preTrainedModel().eval().cuda()\n",
        "#vgg16 = Vgg16().eval().cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /content/.torch/models/vgg16-397923af.pth\n",
            "1.2%"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "28.5%"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "DsoOKn4BTv_F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8cde47eb-6114-4ccb-9df9-11c90628f1a1"
      },
      "cell_type": "code",
      "source": [
        "#Get the CIFAR10 train images \n",
        "cifar = datasets.CIFAR10('./data/cifar/', train = True, download = True)\n",
        "\n",
        "# Organize training data in batches, \n",
        "# normalize them to have values between [-1, 1] (?)\n",
        "\n",
        "train_images = torch.utils.data.DataLoader ( datasets.CIFAR10('./data/cifar/', train = True, download=False,\n",
        "                               transform=transforms.Compose([\n",
        "                               #transforms.Resize(64), \n",
        "                               #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                               transforms.ToTensor(),])) , \n",
        "                               batch_size = 4, shuffle = True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0N6c7NtWTv_Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0909be8b-1d6e-48b4-957a-1bb41c8c8220"
      },
      "cell_type": "code",
      "source": [
        "upsampling = nn.Upsample(size=256)\n",
        "for batch_idx, (data,_) in enumerate(train_images):    \n",
        "    out = vgg16(upsampling(data.cuda()))\n",
        "    \n",
        "    print('Data size: ', data.size())\n",
        "    print('Output 1 size: ', out[0].size())\n",
        "    print('Output 2 size: ', out[1].size())\n",
        "    #print('Output 3 size: ', out[2].size())\n",
        "    #print('Output 4 size: ', out[3].size())\n",
        "    \n",
        "    break"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data size:  torch.Size([4, 3, 32, 32])\n",
            "Output 1 size:  torch.Size([64, 256, 256])\n",
            "Output 2 size:  torch.Size([64, 256, 256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AsFG7sSkTv_d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "We will use the arquitecture suggested by [Radford et al](https://arxiv.org/abs/1511.06434) for both the encoder and decoder. With convolutional layers in the encoder and fractionally-strided  convolutions  in  the  decoder.   In  each convolutional layer in the encoder we double the number of filters present in the previous layer and use a convolutional stride of 2.  In each convolutional layer in the decoder we use a fractional stride of 2 and halve the number of filters on each layer."
      ]
    },
    {
      "metadata": {
        "id": "BxB4_CTcTv_d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class VAE( nn.Module ):\n",
        "\n",
        "    def __init__ ( self, image_size ,  hidden_dim , encoding_dim ):\n",
        "        \n",
        "        super( VAE, self ).__init__()\n",
        "        \n",
        "        self.encoding_dim = encoding_dim\n",
        "        self.image_size = image_size\n",
        "        self.hidden_dim = hidden_dim \n",
        "        \n",
        "        # Decoder - Fractional strided convolutional layers\n",
        "        self.decoder  = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 4, 1, 0, bias = False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(32, 3, 4, 2, 1, bias = False),\n",
        "            nn.Sigmoid() # nn.Tanh()  \n",
        "        )\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, 2, 1, bias = False),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(32, 64, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 0, bias = False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "        # Fully-connected layers\n",
        "        self.fc1 = nn.Linear(256, self.hidden_dim)\n",
        "        self.fc21 = nn.Linear(self.hidden_dim, self.encoding_dim)\n",
        "        self.fc22 = nn.Linear(self.hidden_dim, self.encoding_dim)\n",
        "        self.fc3 = nn.Linear(self.encoding_dim, self.hidden_dim)\n",
        "        self.fc4 = nn.Linear(self.hidden_dim, 256)\n",
        "    \n",
        "    def decode (self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        h4 = F.sigmoid(self.fc4(h3))\n",
        "        return self.decoder( h4.view(z.size(0),-1,1,1) ) \n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # Encode \n",
        "        encoded = F.relu(self.fc1( self.encoder(x).view(x.size(0), -1) ) )\n",
        "        \n",
        "        #Obtain mu and logvar\n",
        "        mu = self.fc21( encoded )\n",
        "        logvar = self.fc22 ( encoded )\n",
        "        \n",
        "        #Reparametrization trick\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = eps.mul(std).add_(mu)\n",
        "        \n",
        "        # Decode \n",
        "        decoded = self.decode(z)\n",
        "\n",
        "        # return decoded, mu, logvar\n",
        "        return decoded, mu , logvar\n",
        "\n",
        "\n",
        "upsampling = nn.Upsample(size=256)\n",
        "sigmoid = nn.Sigmoid()\n",
        "\n",
        "# Reconstruction + KL divergence losses summed over all elements and batch\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "  \n",
        "  # VAE LOSS\n",
        "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "  \n",
        "  # PERCEPTUAL LOSS\n",
        "    recon_x_vgg16 = vgg16 ( upsampling( recon_x ) )\n",
        "    x_vgg16 = vgg16 ( upsampling( x ) )\n",
        "    \n",
        "    L0 = F.mse_loss( Srecon_x_vgg16[0] , x_vgg16[0], size_average=False)\n",
        "    L1 = F.mse_loss( recon_x_vgg16[1] , x_vgg16[1], size_average=False)\n",
        "    #L2 = F.mse_loss( recon_x_vgg16[2] , x_vgg16[2], size_average=False)\n",
        "    #L3 = F.mse_loss( recon_x_vgg16[3] , x_vgg16[3], size_average=False)\n",
        "    #print('L0: ',L0.data)\n",
        "    #print('L1:' ,L1.data)\n",
        "    #print('L2: ',L2.data)\n",
        "    #print('L3: ', L3.data)\n",
        "    #print(BCE.data)\n",
        "    #print(KLD.data)\n",
        "    \n",
        "    del recon_x_vgg16 \n",
        "    del x_vgg16\n",
        "    \n",
        "    return BCE + KLD + L0 + L1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xa14nLhnTv_f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Define model\n",
        "model = VAE( 32, 100, 20 ).cuda()\n",
        "#model.load_state_dict(torch.load('save_checkpoint_epoch_9.pth'))\n",
        "  \n",
        "  \n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "#Train model\n",
        "def train(epoch):\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_images):\n",
        "        data = Variable(data).cuda()\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_images.dataset),\n",
        "                100. * batch_idx / len(train_images),\n",
        "                loss.item() / len(data)))\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(train_images.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GJFbZKvNTv_h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9635
        },
        "outputId": "c18ab630-f986-486a-afb9-aa00f1b9ab9f"
      },
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "for epoch in range(1,num_epochs):\n",
        "    train(epoch)    "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 116313.093750\n",
            "Train Epoch: 1 [200/50000 (0%)]\tLoss: 177329.093750\n",
            "Train Epoch: 1 [400/50000 (1%)]\tLoss: 90363.796875\n",
            "Train Epoch: 1 [600/50000 (1%)]\tLoss: 88329.031250\n",
            "Train Epoch: 1 [800/50000 (2%)]\tLoss: 111293.375000\n",
            "Train Epoch: 1 [1000/50000 (2%)]\tLoss: 76695.054688\n",
            "Train Epoch: 1 [1200/50000 (2%)]\tLoss: 139406.093750\n",
            "Train Epoch: 1 [1400/50000 (3%)]\tLoss: 85086.632812\n",
            "Train Epoch: 1 [1600/50000 (3%)]\tLoss: 55279.351562\n",
            "Train Epoch: 1 [1800/50000 (4%)]\tLoss: 86910.640625\n",
            "Train Epoch: 1 [2000/50000 (4%)]\tLoss: 65911.195312\n",
            "Train Epoch: 1 [2200/50000 (4%)]\tLoss: 133015.531250\n",
            "Train Epoch: 1 [2400/50000 (5%)]\tLoss: 77796.062500\n",
            "Train Epoch: 1 [2600/50000 (5%)]\tLoss: 92703.843750\n",
            "Train Epoch: 1 [2800/50000 (6%)]\tLoss: 97212.656250\n",
            "Train Epoch: 1 [3000/50000 (6%)]\tLoss: 122148.476562\n",
            "Train Epoch: 1 [3200/50000 (6%)]\tLoss: 133246.968750\n",
            "Train Epoch: 1 [3400/50000 (7%)]\tLoss: 73505.500000\n",
            "Train Epoch: 1 [3600/50000 (7%)]\tLoss: 64181.964844\n",
            "Train Epoch: 1 [3800/50000 (8%)]\tLoss: 50667.761719\n",
            "Train Epoch: 1 [4000/50000 (8%)]\tLoss: 105816.515625\n",
            "Train Epoch: 1 [4200/50000 (8%)]\tLoss: 157954.312500\n",
            "Train Epoch: 1 [4400/50000 (9%)]\tLoss: 124866.023438\n",
            "Train Epoch: 1 [4600/50000 (9%)]\tLoss: 114764.171875\n",
            "Train Epoch: 1 [4800/50000 (10%)]\tLoss: 38958.757812\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tLoss: 116727.632812\n",
            "Train Epoch: 1 [5200/50000 (10%)]\tLoss: 68931.820312\n",
            "Train Epoch: 1 [5400/50000 (11%)]\tLoss: 89874.132812\n",
            "Train Epoch: 1 [5600/50000 (11%)]\tLoss: 96488.921875\n",
            "Train Epoch: 1 [5800/50000 (12%)]\tLoss: 90040.578125\n",
            "Train Epoch: 1 [6000/50000 (12%)]\tLoss: 128815.406250\n",
            "Train Epoch: 1 [6200/50000 (12%)]\tLoss: 105734.343750\n",
            "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 104370.671875\n",
            "Train Epoch: 1 [6600/50000 (13%)]\tLoss: 93359.000000\n",
            "Train Epoch: 1 [6800/50000 (14%)]\tLoss: 73216.500000\n",
            "Train Epoch: 1 [7000/50000 (14%)]\tLoss: 98943.328125\n",
            "Train Epoch: 1 [7200/50000 (14%)]\tLoss: 85069.757812\n",
            "Train Epoch: 1 [7400/50000 (15%)]\tLoss: 105980.890625\n",
            "Train Epoch: 1 [7600/50000 (15%)]\tLoss: 95046.093750\n",
            "Train Epoch: 1 [7800/50000 (16%)]\tLoss: 117129.601562\n",
            "Train Epoch: 1 [8000/50000 (16%)]\tLoss: 93120.515625\n",
            "Train Epoch: 1 [8200/50000 (16%)]\tLoss: 94755.164062\n",
            "Train Epoch: 1 [8400/50000 (17%)]\tLoss: 100828.046875\n",
            "Train Epoch: 1 [8600/50000 (17%)]\tLoss: 86998.109375\n",
            "Train Epoch: 1 [8800/50000 (18%)]\tLoss: 104762.343750\n",
            "Train Epoch: 1 [9000/50000 (18%)]\tLoss: 108149.968750\n",
            "Train Epoch: 1 [9200/50000 (18%)]\tLoss: 109508.578125\n",
            "Train Epoch: 1 [9400/50000 (19%)]\tLoss: 75978.679688\n",
            "Train Epoch: 1 [9600/50000 (19%)]\tLoss: 107130.062500\n",
            "Train Epoch: 1 [9800/50000 (20%)]\tLoss: 84097.835938\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tLoss: 128886.171875\n",
            "Train Epoch: 1 [10200/50000 (20%)]\tLoss: 86257.195312\n",
            "Train Epoch: 1 [10400/50000 (21%)]\tLoss: 119882.781250\n",
            "Train Epoch: 1 [10600/50000 (21%)]\tLoss: 98308.093750\n",
            "Train Epoch: 1 [10800/50000 (22%)]\tLoss: 69364.445312\n",
            "Train Epoch: 1 [11000/50000 (22%)]\tLoss: 77386.921875\n",
            "Train Epoch: 1 [11200/50000 (22%)]\tLoss: 93732.140625\n",
            "Train Epoch: 1 [11400/50000 (23%)]\tLoss: 158319.140625\n",
            "Train Epoch: 1 [11600/50000 (23%)]\tLoss: 123540.421875\n",
            "Train Epoch: 1 [11800/50000 (24%)]\tLoss: 120710.390625\n",
            "Train Epoch: 1 [12000/50000 (24%)]\tLoss: 98114.320312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [12200/50000 (24%)]\tLoss: 107481.109375\n",
            "Train Epoch: 1 [12400/50000 (25%)]\tLoss: 107772.062500\n",
            "Train Epoch: 1 [12600/50000 (25%)]\tLoss: 166984.593750\n",
            "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 135839.656250\n",
            "Train Epoch: 1 [13000/50000 (26%)]\tLoss: 97730.218750\n",
            "Train Epoch: 1 [13200/50000 (26%)]\tLoss: 75957.367188\n",
            "Train Epoch: 1 [13400/50000 (27%)]\tLoss: 107419.296875\n",
            "Train Epoch: 1 [13600/50000 (27%)]\tLoss: 81190.890625\n",
            "Train Epoch: 1 [13800/50000 (28%)]\tLoss: 89178.382812\n",
            "Train Epoch: 1 [14000/50000 (28%)]\tLoss: 81774.093750\n",
            "Train Epoch: 1 [14200/50000 (28%)]\tLoss: 131574.390625\n",
            "Train Epoch: 1 [14400/50000 (29%)]\tLoss: 135029.921875\n",
            "Train Epoch: 1 [14600/50000 (29%)]\tLoss: 95796.796875\n",
            "Train Epoch: 1 [14800/50000 (30%)]\tLoss: 140830.687500\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tLoss: 107763.375000\n",
            "Train Epoch: 1 [15200/50000 (30%)]\tLoss: 127720.281250\n",
            "Train Epoch: 1 [15400/50000 (31%)]\tLoss: 94280.796875\n",
            "Train Epoch: 1 [15600/50000 (31%)]\tLoss: 87144.937500\n",
            "Train Epoch: 1 [15800/50000 (32%)]\tLoss: 53327.625000\n",
            "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 75839.546875\n",
            "Train Epoch: 1 [16200/50000 (32%)]\tLoss: 88488.515625\n",
            "Train Epoch: 1 [16400/50000 (33%)]\tLoss: 68856.968750\n",
            "Train Epoch: 1 [16600/50000 (33%)]\tLoss: 52603.281250\n",
            "Train Epoch: 1 [16800/50000 (34%)]\tLoss: 83236.351562\n",
            "Train Epoch: 1 [17000/50000 (34%)]\tLoss: 86447.773438\n",
            "Train Epoch: 1 [17200/50000 (34%)]\tLoss: 95802.289062\n",
            "Train Epoch: 1 [17400/50000 (35%)]\tLoss: 72797.531250\n",
            "Train Epoch: 1 [17600/50000 (35%)]\tLoss: 131353.234375\n",
            "Train Epoch: 1 [17800/50000 (36%)]\tLoss: 106610.171875\n",
            "Train Epoch: 1 [18000/50000 (36%)]\tLoss: 124756.468750\n",
            "Train Epoch: 1 [18200/50000 (36%)]\tLoss: 103762.906250\n",
            "Train Epoch: 1 [18400/50000 (37%)]\tLoss: 51534.968750\n",
            "Train Epoch: 1 [18600/50000 (37%)]\tLoss: 53171.144531\n",
            "Train Epoch: 1 [18800/50000 (38%)]\tLoss: 88598.390625\n",
            "Train Epoch: 1 [19000/50000 (38%)]\tLoss: 85483.062500\n",
            "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 147646.562500\n",
            "Train Epoch: 1 [19400/50000 (39%)]\tLoss: 62826.531250\n",
            "Train Epoch: 1 [19600/50000 (39%)]\tLoss: 99657.953125\n",
            "Train Epoch: 1 [19800/50000 (40%)]\tLoss: 73960.976562\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tLoss: 104731.859375\n",
            "Train Epoch: 1 [20200/50000 (40%)]\tLoss: 109205.367188\n",
            "Train Epoch: 1 [20400/50000 (41%)]\tLoss: 121684.453125\n",
            "Train Epoch: 1 [20600/50000 (41%)]\tLoss: 68147.335938\n",
            "Train Epoch: 1 [20800/50000 (42%)]\tLoss: 77436.179688\n",
            "Train Epoch: 1 [21000/50000 (42%)]\tLoss: 102575.625000\n",
            "Train Epoch: 1 [21200/50000 (42%)]\tLoss: 122758.640625\n",
            "Train Epoch: 1 [21400/50000 (43%)]\tLoss: 108491.531250\n",
            "Train Epoch: 1 [21600/50000 (43%)]\tLoss: 99374.765625\n",
            "Train Epoch: 1 [21800/50000 (44%)]\tLoss: 92259.312500\n",
            "Train Epoch: 1 [22000/50000 (44%)]\tLoss: 110984.937500\n",
            "Train Epoch: 1 [22200/50000 (44%)]\tLoss: 69996.500000\n",
            "Train Epoch: 1 [22400/50000 (45%)]\tLoss: 59023.980469\n",
            "Train Epoch: 1 [22600/50000 (45%)]\tLoss: 107100.734375\n",
            "Train Epoch: 1 [22800/50000 (46%)]\tLoss: 80632.640625\n",
            "Train Epoch: 1 [23000/50000 (46%)]\tLoss: 93937.351562\n",
            "Train Epoch: 1 [23200/50000 (46%)]\tLoss: 123927.781250\n",
            "Train Epoch: 1 [23400/50000 (47%)]\tLoss: 62884.746094\n",
            "Train Epoch: 1 [23600/50000 (47%)]\tLoss: 91238.593750\n",
            "Train Epoch: 1 [23800/50000 (48%)]\tLoss: 88551.203125\n",
            "Train Epoch: 1 [24000/50000 (48%)]\tLoss: 62778.539062\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [24200/50000 (48%)]\tLoss: 125420.453125\n",
            "Train Epoch: 1 [24400/50000 (49%)]\tLoss: 95299.570312\n",
            "Train Epoch: 1 [24600/50000 (49%)]\tLoss: 102346.015625\n",
            "Train Epoch: 1 [24800/50000 (50%)]\tLoss: 107715.507812\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tLoss: 93819.921875\n",
            "Train Epoch: 1 [25200/50000 (50%)]\tLoss: 135878.765625\n",
            "Train Epoch: 1 [25400/50000 (51%)]\tLoss: 141209.437500\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 103624.539062\n",
            "Train Epoch: 1 [25800/50000 (52%)]\tLoss: 76942.562500\n",
            "Train Epoch: 1 [26000/50000 (52%)]\tLoss: 91740.523438\n",
            "Train Epoch: 1 [26200/50000 (52%)]\tLoss: 143995.593750\n",
            "Train Epoch: 1 [26400/50000 (53%)]\tLoss: 82297.437500\n",
            "Train Epoch: 1 [26600/50000 (53%)]\tLoss: 110785.898438\n",
            "Train Epoch: 1 [26800/50000 (54%)]\tLoss: 81875.210938\n",
            "Train Epoch: 1 [27000/50000 (54%)]\tLoss: 71414.007812\n",
            "Train Epoch: 1 [27200/50000 (54%)]\tLoss: 71028.804688\n",
            "Train Epoch: 1 [27400/50000 (55%)]\tLoss: 115013.929688\n",
            "Train Epoch: 1 [27600/50000 (55%)]\tLoss: 93134.234375\n",
            "Train Epoch: 1 [27800/50000 (56%)]\tLoss: 103971.859375\n",
            "Train Epoch: 1 [28000/50000 (56%)]\tLoss: 89249.906250\n",
            "Train Epoch: 1 [28200/50000 (56%)]\tLoss: 97402.734375\n",
            "Train Epoch: 1 [28400/50000 (57%)]\tLoss: 83252.007812\n",
            "Train Epoch: 1 [28600/50000 (57%)]\tLoss: 103719.625000\n",
            "Train Epoch: 1 [28800/50000 (58%)]\tLoss: 61192.550781\n",
            "Train Epoch: 1 [29000/50000 (58%)]\tLoss: 94634.367188\n",
            "Train Epoch: 1 [29200/50000 (58%)]\tLoss: 121339.203125\n",
            "Train Epoch: 1 [29400/50000 (59%)]\tLoss: 58016.394531\n",
            "Train Epoch: 1 [29600/50000 (59%)]\tLoss: 123826.062500\n",
            "Train Epoch: 1 [29800/50000 (60%)]\tLoss: 106364.625000\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tLoss: 75889.000000\n",
            "Train Epoch: 1 [30200/50000 (60%)]\tLoss: 83612.703125\n",
            "Train Epoch: 1 [30400/50000 (61%)]\tLoss: 102873.484375\n",
            "Train Epoch: 1 [30600/50000 (61%)]\tLoss: 77531.382812\n",
            "Train Epoch: 1 [30800/50000 (62%)]\tLoss: 91663.000000\n",
            "Train Epoch: 1 [31000/50000 (62%)]\tLoss: 81691.812500\n",
            "Train Epoch: 1 [31200/50000 (62%)]\tLoss: 94560.078125\n",
            "Train Epoch: 1 [31400/50000 (63%)]\tLoss: 91456.500000\n",
            "Train Epoch: 1 [31600/50000 (63%)]\tLoss: 91297.484375\n",
            "Train Epoch: 1 [31800/50000 (64%)]\tLoss: 118361.546875\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 68677.437500\n",
            "Train Epoch: 1 [32200/50000 (64%)]\tLoss: 87284.484375\n",
            "Train Epoch: 1 [32400/50000 (65%)]\tLoss: 116389.968750\n",
            "Train Epoch: 1 [32600/50000 (65%)]\tLoss: 77098.601562\n",
            "Train Epoch: 1 [32800/50000 (66%)]\tLoss: 141716.656250\n",
            "Train Epoch: 1 [33000/50000 (66%)]\tLoss: 78868.640625\n",
            "Train Epoch: 1 [33200/50000 (66%)]\tLoss: 99725.750000\n",
            "Train Epoch: 1 [33400/50000 (67%)]\tLoss: 124980.968750\n",
            "Train Epoch: 1 [33600/50000 (67%)]\tLoss: 83054.562500\n",
            "Train Epoch: 1 [33800/50000 (68%)]\tLoss: 91253.609375\n",
            "Train Epoch: 1 [34000/50000 (68%)]\tLoss: 118219.265625\n",
            "Train Epoch: 1 [34200/50000 (68%)]\tLoss: 85012.992188\n",
            "Train Epoch: 1 [34400/50000 (69%)]\tLoss: 82619.382812\n",
            "Train Epoch: 1 [34600/50000 (69%)]\tLoss: 92561.453125\n",
            "Train Epoch: 1 [34800/50000 (70%)]\tLoss: 58060.089844\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tLoss: 99553.328125\n",
            "Train Epoch: 1 [35200/50000 (70%)]\tLoss: 70103.757812\n",
            "Train Epoch: 1 [35400/50000 (71%)]\tLoss: 124819.546875\n",
            "Train Epoch: 1 [35600/50000 (71%)]\tLoss: 81483.265625\n",
            "Train Epoch: 1 [35800/50000 (72%)]\tLoss: 101047.898438\n",
            "Train Epoch: 1 [36000/50000 (72%)]\tLoss: 85301.187500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [36200/50000 (72%)]\tLoss: 89195.257812\n",
            "Train Epoch: 1 [36400/50000 (73%)]\tLoss: 89470.140625\n",
            "Train Epoch: 1 [36600/50000 (73%)]\tLoss: 138879.531250\n",
            "Train Epoch: 1 [36800/50000 (74%)]\tLoss: 108383.937500\n",
            "Train Epoch: 1 [37000/50000 (74%)]\tLoss: 94111.523438\n",
            "Train Epoch: 1 [37200/50000 (74%)]\tLoss: 86102.828125\n",
            "Train Epoch: 1 [37400/50000 (75%)]\tLoss: 76752.507812\n",
            "Train Epoch: 1 [37600/50000 (75%)]\tLoss: 91309.976562\n",
            "Train Epoch: 1 [37800/50000 (76%)]\tLoss: 49155.527344\n",
            "Train Epoch: 1 [38000/50000 (76%)]\tLoss: 148911.562500\n",
            "Train Epoch: 1 [38200/50000 (76%)]\tLoss: 97301.703125\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 88891.078125\n",
            "Train Epoch: 1 [38600/50000 (77%)]\tLoss: 160911.015625\n",
            "Train Epoch: 1 [38800/50000 (78%)]\tLoss: 79497.078125\n",
            "Train Epoch: 1 [39000/50000 (78%)]\tLoss: 96868.234375\n",
            "Train Epoch: 1 [39200/50000 (78%)]\tLoss: 207569.593750\n",
            "Train Epoch: 1 [39400/50000 (79%)]\tLoss: 59324.382812\n",
            "Train Epoch: 1 [39600/50000 (79%)]\tLoss: 78708.945312\n",
            "Train Epoch: 1 [39800/50000 (80%)]\tLoss: 63831.945312\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tLoss: 177038.031250\n",
            "Train Epoch: 1 [40200/50000 (80%)]\tLoss: 101223.804688\n",
            "Train Epoch: 1 [40400/50000 (81%)]\tLoss: 69417.484375\n",
            "Train Epoch: 1 [40600/50000 (81%)]\tLoss: 69199.789062\n",
            "Train Epoch: 1 [40800/50000 (82%)]\tLoss: 127443.867188\n",
            "Train Epoch: 1 [41000/50000 (82%)]\tLoss: 88157.195312\n",
            "Train Epoch: 1 [41200/50000 (82%)]\tLoss: 63746.070312\n",
            "Train Epoch: 1 [41400/50000 (83%)]\tLoss: 104638.265625\n",
            "Train Epoch: 1 [41600/50000 (83%)]\tLoss: 76673.179688\n",
            "Train Epoch: 1 [41800/50000 (84%)]\tLoss: 61484.734375\n",
            "Train Epoch: 1 [42000/50000 (84%)]\tLoss: 76509.054688\n",
            "Train Epoch: 1 [42200/50000 (84%)]\tLoss: 109563.796875\n",
            "Train Epoch: 1 [42400/50000 (85%)]\tLoss: 90584.375000\n",
            "Train Epoch: 1 [42600/50000 (85%)]\tLoss: 76159.351562\n",
            "Train Epoch: 1 [42800/50000 (86%)]\tLoss: 85327.390625\n",
            "Train Epoch: 1 [43000/50000 (86%)]\tLoss: 94573.390625\n",
            "Train Epoch: 1 [43200/50000 (86%)]\tLoss: 146075.546875\n",
            "Train Epoch: 1 [43400/50000 (87%)]\tLoss: 78823.789062\n",
            "Train Epoch: 1 [43600/50000 (87%)]\tLoss: 55704.109375\n",
            "Train Epoch: 1 [43800/50000 (88%)]\tLoss: 115645.031250\n",
            "Train Epoch: 1 [44000/50000 (88%)]\tLoss: 70157.093750\n",
            "Train Epoch: 1 [44200/50000 (88%)]\tLoss: 100167.914062\n",
            "Train Epoch: 1 [44400/50000 (89%)]\tLoss: 74422.367188\n",
            "Train Epoch: 1 [44600/50000 (89%)]\tLoss: 103110.601562\n",
            "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 75618.781250\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tLoss: 92889.234375\n",
            "Train Epoch: 1 [45200/50000 (90%)]\tLoss: 76981.578125\n",
            "Train Epoch: 1 [45400/50000 (91%)]\tLoss: 68673.875000\n",
            "Train Epoch: 1 [45600/50000 (91%)]\tLoss: 45939.765625\n",
            "Train Epoch: 1 [45800/50000 (92%)]\tLoss: 67197.546875\n",
            "Train Epoch: 1 [46000/50000 (92%)]\tLoss: 78288.039062\n",
            "Train Epoch: 1 [46200/50000 (92%)]\tLoss: 106226.812500\n",
            "Train Epoch: 1 [46400/50000 (93%)]\tLoss: 81581.156250\n",
            "Train Epoch: 1 [46600/50000 (93%)]\tLoss: 87994.054688\n",
            "Train Epoch: 1 [46800/50000 (94%)]\tLoss: 81184.593750\n",
            "Train Epoch: 1 [47000/50000 (94%)]\tLoss: 102186.335938\n",
            "Train Epoch: 1 [47200/50000 (94%)]\tLoss: 76839.421875\n",
            "Train Epoch: 1 [47400/50000 (95%)]\tLoss: 122593.632812\n",
            "Train Epoch: 1 [47600/50000 (95%)]\tLoss: 131900.765625\n",
            "Train Epoch: 1 [47800/50000 (96%)]\tLoss: 82415.625000\n",
            "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 92809.632812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [48200/50000 (96%)]\tLoss: 77180.015625\n",
            "Train Epoch: 1 [48400/50000 (97%)]\tLoss: 46507.210938\n",
            "Train Epoch: 1 [48600/50000 (97%)]\tLoss: 114034.906250\n",
            "Train Epoch: 1 [48800/50000 (98%)]\tLoss: 64741.429688\n",
            "Train Epoch: 1 [49000/50000 (98%)]\tLoss: 106623.375000\n",
            "Train Epoch: 1 [49200/50000 (98%)]\tLoss: 128692.343750\n",
            "Train Epoch: 1 [49400/50000 (99%)]\tLoss: 91043.085938\n",
            "Train Epoch: 1 [49600/50000 (99%)]\tLoss: 92764.007812\n",
            "Train Epoch: 1 [49800/50000 (100%)]\tLoss: 57108.613281\n",
            "====> Epoch: 1 Average loss: 96331.6684\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 91829.179688\n",
            "Train Epoch: 2 [200/50000 (0%)]\tLoss: 74270.304688\n",
            "Train Epoch: 2 [400/50000 (1%)]\tLoss: 84665.570312\n",
            "Train Epoch: 2 [600/50000 (1%)]\tLoss: 98339.796875\n",
            "Train Epoch: 2 [800/50000 (2%)]\tLoss: 84191.031250\n",
            "Train Epoch: 2 [1000/50000 (2%)]\tLoss: 95502.359375\n",
            "Train Epoch: 2 [1200/50000 (2%)]\tLoss: 73450.984375\n",
            "Train Epoch: 2 [1400/50000 (3%)]\tLoss: 81611.812500\n",
            "Train Epoch: 2 [1600/50000 (3%)]\tLoss: 81013.109375\n",
            "Train Epoch: 2 [1800/50000 (4%)]\tLoss: 104132.914062\n",
            "Train Epoch: 2 [2000/50000 (4%)]\tLoss: 113548.140625\n",
            "Train Epoch: 2 [2200/50000 (4%)]\tLoss: 101295.203125\n",
            "Train Epoch: 2 [2400/50000 (5%)]\tLoss: 132794.796875\n",
            "Train Epoch: 2 [2600/50000 (5%)]\tLoss: 85321.406250\n",
            "Train Epoch: 2 [2800/50000 (6%)]\tLoss: 69245.078125\n",
            "Train Epoch: 2 [3000/50000 (6%)]\tLoss: 58854.085938\n",
            "Train Epoch: 2 [3200/50000 (6%)]\tLoss: 71421.578125\n",
            "Train Epoch: 2 [3400/50000 (7%)]\tLoss: 77825.304688\n",
            "Train Epoch: 2 [3600/50000 (7%)]\tLoss: 78495.750000\n",
            "Train Epoch: 2 [3800/50000 (8%)]\tLoss: 151333.343750\n",
            "Train Epoch: 2 [4000/50000 (8%)]\tLoss: 50531.386719\n",
            "Train Epoch: 2 [4200/50000 (8%)]\tLoss: 127313.578125\n",
            "Train Epoch: 2 [4400/50000 (9%)]\tLoss: 112143.640625\n",
            "Train Epoch: 2 [4600/50000 (9%)]\tLoss: 102232.343750\n",
            "Train Epoch: 2 [4800/50000 (10%)]\tLoss: 131697.796875\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tLoss: 112943.843750\n",
            "Train Epoch: 2 [5200/50000 (10%)]\tLoss: 99668.281250\n",
            "Train Epoch: 2 [5400/50000 (11%)]\tLoss: 103838.531250\n",
            "Train Epoch: 2 [5600/50000 (11%)]\tLoss: 93212.914062\n",
            "Train Epoch: 2 [5800/50000 (12%)]\tLoss: 87203.140625\n",
            "Train Epoch: 2 [6000/50000 (12%)]\tLoss: 114543.890625\n",
            "Train Epoch: 2 [6200/50000 (12%)]\tLoss: 60186.960938\n",
            "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 66152.437500\n",
            "Train Epoch: 2 [6600/50000 (13%)]\tLoss: 95472.375000\n",
            "Train Epoch: 2 [6800/50000 (14%)]\tLoss: 84437.593750\n",
            "Train Epoch: 2 [7000/50000 (14%)]\tLoss: 76486.304688\n",
            "Train Epoch: 2 [7200/50000 (14%)]\tLoss: 122663.023438\n",
            "Train Epoch: 2 [7400/50000 (15%)]\tLoss: 109879.859375\n",
            "Train Epoch: 2 [7600/50000 (15%)]\tLoss: 129177.093750\n",
            "Train Epoch: 2 [7800/50000 (16%)]\tLoss: 69187.000000\n",
            "Train Epoch: 2 [8000/50000 (16%)]\tLoss: 50676.613281\n",
            "Train Epoch: 2 [8200/50000 (16%)]\tLoss: 107555.476562\n",
            "Train Epoch: 2 [8400/50000 (17%)]\tLoss: 95792.765625\n",
            "Train Epoch: 2 [8600/50000 (17%)]\tLoss: 117273.406250\n",
            "Train Epoch: 2 [8800/50000 (18%)]\tLoss: 93474.414062\n",
            "Train Epoch: 2 [9000/50000 (18%)]\tLoss: 88344.984375\n",
            "Train Epoch: 2 [9200/50000 (18%)]\tLoss: 83930.734375\n",
            "Train Epoch: 2 [9400/50000 (19%)]\tLoss: 86300.171875\n",
            "Train Epoch: 2 [9600/50000 (19%)]\tLoss: 118231.109375\n",
            "Train Epoch: 2 [9800/50000 (20%)]\tLoss: 84958.875000\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tLoss: 67850.687500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 2 [10200/50000 (20%)]\tLoss: 137283.453125\n",
            "Train Epoch: 2 [10400/50000 (21%)]\tLoss: 81030.531250\n",
            "Train Epoch: 2 [10600/50000 (21%)]\tLoss: 84680.968750\n",
            "Train Epoch: 2 [10800/50000 (22%)]\tLoss: 81813.000000\n",
            "Train Epoch: 2 [11000/50000 (22%)]\tLoss: 47717.761719\n",
            "Train Epoch: 2 [11200/50000 (22%)]\tLoss: 126276.984375\n",
            "Train Epoch: 2 [11400/50000 (23%)]\tLoss: 132691.578125\n",
            "Train Epoch: 2 [11600/50000 (23%)]\tLoss: 146149.375000\n",
            "Train Epoch: 2 [11800/50000 (24%)]\tLoss: 96342.937500\n",
            "Train Epoch: 2 [12000/50000 (24%)]\tLoss: 96670.000000\n",
            "Train Epoch: 2 [12200/50000 (24%)]\tLoss: 78887.460938\n",
            "Train Epoch: 2 [12400/50000 (25%)]\tLoss: 167553.343750\n",
            "Train Epoch: 2 [12600/50000 (25%)]\tLoss: 88895.687500\n",
            "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 86295.585938\n",
            "Train Epoch: 2 [13000/50000 (26%)]\tLoss: 90745.718750\n",
            "Train Epoch: 2 [13200/50000 (26%)]\tLoss: 88119.125000\n",
            "Train Epoch: 2 [13400/50000 (27%)]\tLoss: 66335.500000\n",
            "Train Epoch: 2 [13600/50000 (27%)]\tLoss: 143227.484375\n",
            "Train Epoch: 2 [13800/50000 (28%)]\tLoss: 47426.109375\n",
            "Train Epoch: 2 [14000/50000 (28%)]\tLoss: 88832.359375\n",
            "Train Epoch: 2 [14200/50000 (28%)]\tLoss: 115742.718750\n",
            "Train Epoch: 2 [14400/50000 (29%)]\tLoss: 68748.078125\n",
            "Train Epoch: 2 [14600/50000 (29%)]\tLoss: 81287.421875\n",
            "Train Epoch: 2 [14800/50000 (30%)]\tLoss: 88873.875000\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tLoss: 79592.484375\n",
            "Train Epoch: 2 [15200/50000 (30%)]\tLoss: 90680.945312\n",
            "Train Epoch: 2 [15400/50000 (31%)]\tLoss: 131608.734375\n",
            "Train Epoch: 2 [15600/50000 (31%)]\tLoss: 101105.484375\n",
            "Train Epoch: 2 [15800/50000 (32%)]\tLoss: 109681.609375\n",
            "Train Epoch: 2 [16000/50000 (32%)]\tLoss: 141282.109375\n",
            "Train Epoch: 2 [16200/50000 (32%)]\tLoss: 76245.382812\n",
            "Train Epoch: 2 [16400/50000 (33%)]\tLoss: 81197.625000\n",
            "Train Epoch: 2 [16600/50000 (33%)]\tLoss: 132938.781250\n",
            "Train Epoch: 2 [16800/50000 (34%)]\tLoss: 122825.992188\n",
            "Train Epoch: 2 [17000/50000 (34%)]\tLoss: 57697.089844\n",
            "Train Epoch: 2 [17200/50000 (34%)]\tLoss: 91282.750000\n",
            "Train Epoch: 2 [17400/50000 (35%)]\tLoss: 122598.062500\n",
            "Train Epoch: 2 [17600/50000 (35%)]\tLoss: 112393.523438\n",
            "Train Epoch: 2 [17800/50000 (36%)]\tLoss: 112919.531250\n",
            "Train Epoch: 2 [18000/50000 (36%)]\tLoss: 111845.875000\n",
            "Train Epoch: 2 [18200/50000 (36%)]\tLoss: 110939.546875\n",
            "Train Epoch: 2 [18400/50000 (37%)]\tLoss: 128271.703125\n",
            "Train Epoch: 2 [18600/50000 (37%)]\tLoss: 101186.546875\n",
            "Train Epoch: 2 [18800/50000 (38%)]\tLoss: 119194.796875\n",
            "Train Epoch: 2 [19000/50000 (38%)]\tLoss: 116197.156250\n",
            "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 76827.437500\n",
            "Train Epoch: 2 [19400/50000 (39%)]\tLoss: 81953.718750\n",
            "Train Epoch: 2 [19600/50000 (39%)]\tLoss: 62080.414062\n",
            "Train Epoch: 2 [19800/50000 (40%)]\tLoss: 87845.109375\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tLoss: 97838.742188\n",
            "Train Epoch: 2 [20200/50000 (40%)]\tLoss: 85886.789062\n",
            "Train Epoch: 2 [20400/50000 (41%)]\tLoss: 144238.968750\n",
            "Train Epoch: 2 [20600/50000 (41%)]\tLoss: 88313.062500\n",
            "Train Epoch: 2 [20800/50000 (42%)]\tLoss: 72900.703125\n",
            "Train Epoch: 2 [21000/50000 (42%)]\tLoss: 55587.007812\n",
            "Train Epoch: 2 [21200/50000 (42%)]\tLoss: 93884.171875\n",
            "Train Epoch: 2 [21400/50000 (43%)]\tLoss: 153654.968750\n",
            "Train Epoch: 2 [21600/50000 (43%)]\tLoss: 77699.796875\n",
            "Train Epoch: 2 [21800/50000 (44%)]\tLoss: 54142.605469\n",
            "Train Epoch: 2 [22000/50000 (44%)]\tLoss: 152591.156250\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 2 [22200/50000 (44%)]\tLoss: 71285.976562\n",
            "Train Epoch: 2 [22400/50000 (45%)]\tLoss: 94303.609375\n",
            "Train Epoch: 2 [22600/50000 (45%)]\tLoss: 103573.703125\n",
            "Train Epoch: 2 [22800/50000 (46%)]\tLoss: 102350.781250\n",
            "Train Epoch: 2 [23000/50000 (46%)]\tLoss: 108690.851562\n",
            "Train Epoch: 2 [23200/50000 (46%)]\tLoss: 52095.976562\n",
            "Train Epoch: 2 [23400/50000 (47%)]\tLoss: 129563.859375\n",
            "Train Epoch: 2 [23600/50000 (47%)]\tLoss: 81429.625000\n",
            "Train Epoch: 2 [23800/50000 (48%)]\tLoss: 78631.335938\n",
            "Train Epoch: 2 [24000/50000 (48%)]\tLoss: 79730.859375\n",
            "Train Epoch: 2 [24200/50000 (48%)]\tLoss: 117729.445312\n",
            "Train Epoch: 2 [24400/50000 (49%)]\tLoss: 95491.406250\n",
            "Train Epoch: 2 [24600/50000 (49%)]\tLoss: 104785.890625\n",
            "Train Epoch: 2 [24800/50000 (50%)]\tLoss: 129496.179688\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tLoss: 50710.976562\n",
            "Train Epoch: 2 [25200/50000 (50%)]\tLoss: 113850.187500\n",
            "Train Epoch: 2 [25400/50000 (51%)]\tLoss: 91233.015625\n",
            "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 77884.664062\n",
            "Train Epoch: 2 [25800/50000 (52%)]\tLoss: 92494.734375\n",
            "Train Epoch: 2 [26000/50000 (52%)]\tLoss: 47040.039062\n",
            "Train Epoch: 2 [26200/50000 (52%)]\tLoss: 91403.210938\n",
            "Train Epoch: 2 [26400/50000 (53%)]\tLoss: 125650.265625\n",
            "Train Epoch: 2 [26600/50000 (53%)]\tLoss: 33135.902344\n",
            "Train Epoch: 2 [26800/50000 (54%)]\tLoss: 110654.328125\n",
            "Train Epoch: 2 [27000/50000 (54%)]\tLoss: 127044.281250\n",
            "Train Epoch: 2 [27200/50000 (54%)]\tLoss: 68262.742188\n",
            "Train Epoch: 2 [27400/50000 (55%)]\tLoss: 80875.117188\n",
            "Train Epoch: 2 [27600/50000 (55%)]\tLoss: 79345.140625\n",
            "Train Epoch: 2 [27800/50000 (56%)]\tLoss: 118612.093750\n",
            "Train Epoch: 2 [28000/50000 (56%)]\tLoss: 120425.703125\n",
            "Train Epoch: 2 [28200/50000 (56%)]\tLoss: 65495.179688\n",
            "Train Epoch: 2 [28400/50000 (57%)]\tLoss: 78968.781250\n",
            "Train Epoch: 2 [28600/50000 (57%)]\tLoss: 114381.429688\n",
            "Train Epoch: 2 [28800/50000 (58%)]\tLoss: 78044.382812\n",
            "Train Epoch: 2 [29000/50000 (58%)]\tLoss: 83518.359375\n",
            "Train Epoch: 2 [29200/50000 (58%)]\tLoss: 66406.289062\n",
            "Train Epoch: 2 [29400/50000 (59%)]\tLoss: 87217.406250\n",
            "Train Epoch: 2 [29600/50000 (59%)]\tLoss: 52364.921875\n",
            "Train Epoch: 2 [29800/50000 (60%)]\tLoss: 87884.765625\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tLoss: 95534.078125\n",
            "Train Epoch: 2 [30200/50000 (60%)]\tLoss: 63807.648438\n",
            "Train Epoch: 2 [30400/50000 (61%)]\tLoss: 52487.957031\n",
            "Train Epoch: 2 [30600/50000 (61%)]\tLoss: 50079.304688\n",
            "Train Epoch: 2 [30800/50000 (62%)]\tLoss: 114572.625000\n",
            "Train Epoch: 2 [31000/50000 (62%)]\tLoss: 74140.968750\n",
            "Train Epoch: 2 [31200/50000 (62%)]\tLoss: 137458.000000\n",
            "Train Epoch: 2 [31400/50000 (63%)]\tLoss: 117362.750000\n",
            "Train Epoch: 2 [31600/50000 (63%)]\tLoss: 118477.765625\n",
            "Train Epoch: 2 [31800/50000 (64%)]\tLoss: 109260.171875\n",
            "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 58490.914062\n",
            "Train Epoch: 2 [32200/50000 (64%)]\tLoss: 88805.437500\n",
            "Train Epoch: 2 [32400/50000 (65%)]\tLoss: 85242.937500\n",
            "Train Epoch: 2 [32600/50000 (65%)]\tLoss: 89155.445312\n",
            "Train Epoch: 2 [32800/50000 (66%)]\tLoss: 67132.242188\n",
            "Train Epoch: 2 [33000/50000 (66%)]\tLoss: 69979.882812\n",
            "Train Epoch: 2 [33200/50000 (66%)]\tLoss: 96618.875000\n",
            "Train Epoch: 2 [33400/50000 (67%)]\tLoss: 99355.304688\n",
            "Train Epoch: 2 [33600/50000 (67%)]\tLoss: 58959.289062\n",
            "Train Epoch: 2 [33800/50000 (68%)]\tLoss: 105320.109375\n",
            "Train Epoch: 2 [34000/50000 (68%)]\tLoss: 80172.703125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 2 [34200/50000 (68%)]\tLoss: 109279.820312\n",
            "Train Epoch: 2 [34400/50000 (69%)]\tLoss: 70319.015625\n",
            "Train Epoch: 2 [34600/50000 (69%)]\tLoss: 120165.421875\n",
            "Train Epoch: 2 [34800/50000 (70%)]\tLoss: 93533.656250\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tLoss: 132103.109375\n",
            "Train Epoch: 2 [35200/50000 (70%)]\tLoss: 52153.328125\n",
            "Train Epoch: 2 [35400/50000 (71%)]\tLoss: 92125.265625\n",
            "Train Epoch: 2 [35600/50000 (71%)]\tLoss: 122928.484375\n",
            "Train Epoch: 2 [35800/50000 (72%)]\tLoss: 105314.484375\n",
            "Train Epoch: 2 [36000/50000 (72%)]\tLoss: 89842.781250\n",
            "Train Epoch: 2 [36200/50000 (72%)]\tLoss: 71909.585938\n",
            "Train Epoch: 2 [36400/50000 (73%)]\tLoss: 77231.414062\n",
            "Train Epoch: 2 [36600/50000 (73%)]\tLoss: 110416.062500\n",
            "Train Epoch: 2 [36800/50000 (74%)]\tLoss: 78321.656250\n",
            "Train Epoch: 2 [37000/50000 (74%)]\tLoss: 60457.781250\n",
            "Train Epoch: 2 [37200/50000 (74%)]\tLoss: 79329.757812\n",
            "Train Epoch: 2 [37400/50000 (75%)]\tLoss: 59211.546875\n",
            "Train Epoch: 2 [37600/50000 (75%)]\tLoss: 62300.406250\n",
            "Train Epoch: 2 [37800/50000 (76%)]\tLoss: 96000.281250\n",
            "Train Epoch: 2 [38000/50000 (76%)]\tLoss: 74718.507812\n",
            "Train Epoch: 2 [38200/50000 (76%)]\tLoss: 79911.046875\n",
            "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 96384.804688\n",
            "Train Epoch: 2 [38600/50000 (77%)]\tLoss: 89683.734375\n",
            "Train Epoch: 2 [38800/50000 (78%)]\tLoss: 133987.609375\n",
            "Train Epoch: 2 [39000/50000 (78%)]\tLoss: 90157.281250\n",
            "Train Epoch: 2 [39200/50000 (78%)]\tLoss: 99141.828125\n",
            "Train Epoch: 2 [39400/50000 (79%)]\tLoss: 102017.750000\n",
            "Train Epoch: 2 [39600/50000 (79%)]\tLoss: 91066.421875\n",
            "Train Epoch: 2 [39800/50000 (80%)]\tLoss: 74501.085938\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tLoss: 97946.281250\n",
            "Train Epoch: 2 [40200/50000 (80%)]\tLoss: 118636.320312\n",
            "Train Epoch: 2 [40400/50000 (81%)]\tLoss: 63323.671875\n",
            "Train Epoch: 2 [40600/50000 (81%)]\tLoss: 74388.054688\n",
            "Train Epoch: 2 [40800/50000 (82%)]\tLoss: 49867.218750\n",
            "Train Epoch: 2 [41000/50000 (82%)]\tLoss: 99359.265625\n",
            "Train Epoch: 2 [41200/50000 (82%)]\tLoss: 91128.007812\n",
            "Train Epoch: 2 [41400/50000 (83%)]\tLoss: 110508.718750\n",
            "Train Epoch: 2 [41600/50000 (83%)]\tLoss: 79388.195312\n",
            "Train Epoch: 2 [41800/50000 (84%)]\tLoss: 91042.070312\n",
            "Train Epoch: 2 [42000/50000 (84%)]\tLoss: 60910.484375\n",
            "Train Epoch: 2 [42200/50000 (84%)]\tLoss: 123525.140625\n",
            "Train Epoch: 2 [42400/50000 (85%)]\tLoss: 67276.710938\n",
            "Train Epoch: 2 [42600/50000 (85%)]\tLoss: 83316.484375\n",
            "Train Epoch: 2 [42800/50000 (86%)]\tLoss: 82891.671875\n",
            "Train Epoch: 2 [43000/50000 (86%)]\tLoss: 106750.328125\n",
            "Train Epoch: 2 [43200/50000 (86%)]\tLoss: 126487.796875\n",
            "Train Epoch: 2 [43400/50000 (87%)]\tLoss: 40807.898438\n",
            "Train Epoch: 2 [43600/50000 (87%)]\tLoss: 108982.328125\n",
            "Train Epoch: 2 [43800/50000 (88%)]\tLoss: 100378.953125\n",
            "Train Epoch: 2 [44000/50000 (88%)]\tLoss: 87108.132812\n",
            "Train Epoch: 2 [44200/50000 (88%)]\tLoss: 158388.609375\n",
            "Train Epoch: 2 [44400/50000 (89%)]\tLoss: 91271.679688\n",
            "Train Epoch: 2 [44600/50000 (89%)]\tLoss: 115980.023438\n",
            "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 95140.460938\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tLoss: 125629.101562\n",
            "Train Epoch: 2 [45200/50000 (90%)]\tLoss: 94902.039062\n",
            "Train Epoch: 2 [45400/50000 (91%)]\tLoss: 68274.203125\n",
            "Train Epoch: 2 [45600/50000 (91%)]\tLoss: 74340.203125\n",
            "Train Epoch: 2 [45800/50000 (92%)]\tLoss: 95363.140625\n",
            "Train Epoch: 2 [46000/50000 (92%)]\tLoss: 90617.601562\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 2 [46200/50000 (92%)]\tLoss: 88385.648438\n",
            "Train Epoch: 2 [46400/50000 (93%)]\tLoss: 83192.679688\n",
            "Train Epoch: 2 [46600/50000 (93%)]\tLoss: 80808.578125\n",
            "Train Epoch: 2 [46800/50000 (94%)]\tLoss: 75559.656250\n",
            "Train Epoch: 2 [47000/50000 (94%)]\tLoss: 145251.109375\n",
            "Train Epoch: 2 [47200/50000 (94%)]\tLoss: 76239.218750\n",
            "Train Epoch: 2 [47400/50000 (95%)]\tLoss: 70946.750000\n",
            "Train Epoch: 2 [47600/50000 (95%)]\tLoss: 69033.109375\n",
            "Train Epoch: 2 [47800/50000 (96%)]\tLoss: 63721.648438\n",
            "Train Epoch: 2 [48000/50000 (96%)]\tLoss: 56590.191406\n",
            "Train Epoch: 2 [48200/50000 (96%)]\tLoss: 74576.726562\n",
            "Train Epoch: 2 [48400/50000 (97%)]\tLoss: 117995.812500\n",
            "Train Epoch: 2 [48600/50000 (97%)]\tLoss: 75117.804688\n",
            "Train Epoch: 2 [48800/50000 (98%)]\tLoss: 98094.875000\n",
            "Train Epoch: 2 [49000/50000 (98%)]\tLoss: 94268.101562\n",
            "Train Epoch: 2 [49200/50000 (98%)]\tLoss: 94207.195312\n",
            "Train Epoch: 2 [49400/50000 (99%)]\tLoss: 47726.941406\n",
            "Train Epoch: 2 [49600/50000 (99%)]\tLoss: 124553.468750\n",
            "Train Epoch: 2 [49800/50000 (100%)]\tLoss: 101148.367188\n",
            "====> Epoch: 2 Average loss: 93180.5951\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 100028.703125\n",
            "Train Epoch: 3 [200/50000 (0%)]\tLoss: 75231.242188\n",
            "Train Epoch: 3 [400/50000 (1%)]\tLoss: 108053.296875\n",
            "Train Epoch: 3 [600/50000 (1%)]\tLoss: 63283.894531\n",
            "Train Epoch: 3 [800/50000 (2%)]\tLoss: 94553.390625\n",
            "Train Epoch: 3 [1000/50000 (2%)]\tLoss: 73327.390625\n",
            "Train Epoch: 3 [1200/50000 (2%)]\tLoss: 87029.218750\n",
            "Train Epoch: 3 [1400/50000 (3%)]\tLoss: 128479.562500\n",
            "Train Epoch: 3 [1600/50000 (3%)]\tLoss: 88040.273438\n",
            "Train Epoch: 3 [1800/50000 (4%)]\tLoss: 85534.593750\n",
            "Train Epoch: 3 [2000/50000 (4%)]\tLoss: 111447.351562\n",
            "Train Epoch: 3 [2200/50000 (4%)]\tLoss: 100830.562500\n",
            "Train Epoch: 3 [2400/50000 (5%)]\tLoss: 118144.937500\n",
            "Train Epoch: 3 [2600/50000 (5%)]\tLoss: 54113.226562\n",
            "Train Epoch: 3 [2800/50000 (6%)]\tLoss: 61323.769531\n",
            "Train Epoch: 3 [3000/50000 (6%)]\tLoss: 100924.781250\n",
            "Train Epoch: 3 [3200/50000 (6%)]\tLoss: 94224.304688\n",
            "Train Epoch: 3 [3400/50000 (7%)]\tLoss: 154211.062500\n",
            "Train Epoch: 3 [3600/50000 (7%)]\tLoss: 93152.906250\n",
            "Train Epoch: 3 [3800/50000 (8%)]\tLoss: 89261.531250\n",
            "Train Epoch: 3 [4000/50000 (8%)]\tLoss: 65758.289062\n",
            "Train Epoch: 3 [4200/50000 (8%)]\tLoss: 76651.515625\n",
            "Train Epoch: 3 [4400/50000 (9%)]\tLoss: 142140.421875\n",
            "Train Epoch: 3 [4600/50000 (9%)]\tLoss: 77317.062500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-1965c095945b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-0ae458329690>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-509b650f3a1d>\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(recon_x, x, mu, logvar)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mx_vgg16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg16\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mupsampling\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mL0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mrecon_x_vgg16\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mx_vgg16\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mL1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mrecon_x_vgg16\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mx_vgg16\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m#L2 = F.mse_loss( recon_x_vgg16[2] , x_vgg16[2], size_average=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce)\u001b[0m\n\u001b[1;32m   1567\u001b[0m     \"\"\"\n\u001b[1;32m   1568\u001b[0m     return _pointwise_loss(lambda a, b: (a - b) ** 2, torch._C._nn.mse_loss,\n\u001b[0;32m-> 1569\u001b[0;31m                            input, target, size_average, reduce)\n\u001b[0m\u001b[1;32m   1570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_pointwise_loss\u001b[0;34m(lambd, lambd_optimized, input, target, size_average, reduce)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlambd_optimized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "wzMjTlShTv_l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "outputId": "5d73cba4-8988-46f7-ddf1-bfe1591291c8"
      },
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "        sample = torch.randn(64, 20).cuda()\n",
        "        sample = model.decode(sample)\n",
        "        torchvision.utils.save_image(sample.view(64, 3, 32, 32),'sample_' + str(epoch) + '.png')\n",
        "        files.download('sample_' + str(epoch) + '.png')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d4157ec613ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sample_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/utils.py\u001b[0m in \u001b[0;36msave_image\u001b[0;34m(tensor, filename, nrow, padding, normalize, range, scale_each, pad_value)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mndarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   1704\u001b[0m         \"\"\"\n\u001b[1;32m   1705\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mresized\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1706\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1707\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mrequested\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpixels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m            \u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mpreinit\u001b[0;34m()\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_initialized\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBmpImagePlugin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/JpegImagePlugin.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTiffImagePlugin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_binary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mi8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi16be\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mi16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mJpegPresets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpresets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1826\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_save_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTiffImageFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_save_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1828\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_extensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTiffImageFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\".tif\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".tiff\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_mime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTiffImageFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"image/tiff\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'PIL.Image' has no attribute 'register_extensions'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "F5A6g4kL2fK8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "torch.save(model.cpu().state_dict(), \"save_checkpoint_epoch_\"+str(epoch)+\".pth\")\n",
        "files.download(\"save_checkpoint_epoch_\"+str(epoch)+\".pth\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_fS8iOSXKpHf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "ada28e72-1b0d-42dc-db0a-08fd5b66f8ee"
      },
      "cell_type": "code",
      "source": [
        "!pip install Pillow==4.0.0\n",
        "!pip install PIL\n",
        "!pip install image\n",
        "\n",
        "sample = torch.randn(64, 20).cuda()\n",
        "sample = model.decode(sample)\n",
        "%matplotlib inline\n",
        "torchvision.utils.save_image(sample.view(64, 3, 32, 32),'sample_' + str(epoch) + '.png')\n",
        "files.download('sample_' + str(epoch) + '.png')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Pillow==4.0.0\n",
            "  Using cached https://files.pythonhosted.org/packages/37/e8/b3fbf87b0188d22246678f8cd61e23e31caa1769ebc06f1664e2e5fe8a17/Pillow-4.0.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow==4.0.0) (0.45.1)\n",
            "\u001b[31mtorchvision 0.2.1 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 5.1.0\n",
            "    Uninstalling Pillow-5.1.0:\n",
            "      Successfully uninstalled Pillow-5.1.0\n",
            "Successfully installed Pillow-4.0.0\n",
            "Collecting PIL\n",
            "\u001b[31m  Could not find a version that satisfies the requirement PIL (from versions: )\u001b[0m\n",
            "\u001b[31mNo matching distribution found for PIL\u001b[0m\n",
            "Requirement already satisfied: image in /usr/local/lib/python3.6/dist-packages (1.5.24)\n",
            "Requirement already satisfied: django in /usr/local/lib/python3.6/dist-packages (from image) (2.0.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from image) (4.0.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from django->image) (2018.4)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->image) (0.45.1)\n",
            "\u001b[31mtorchvision 0.2.1 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XLP47OHlmFyS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}